---
title: "Predicción del rating de Las aplicaciones en Google Play Store"
subtitle: "Reto DataSource"
author: "[Edimer (Sidereus)](https://edimer.github.io/)"
output:
  html_notebook:
    toc: true
    toc_float: 
      smooth_scroll: false
      collapsed: false
    highlight: pygments
    theme: spacelab
    css: estilo.css
    code_folding: show
---

<center>
<img src = "../img/competencia.PNG" />
</center>

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")
```

- [Sitio oficial del reto en DataSource.](https://www.datasource.ai/es/home/data-science-competitions-for-startups/prediciendo-el-rating-de-las-aplicaciones-en-google-play-store)

# Metodología

- Las siguientes imágenes fueron tomadas del libro [Tidy Modeling with R](https://xgboost.readthedocs.io/en/latest/parameter.html) de [Max Kuhn](https://github.com/topepo) y [Julia Silge.](https://github.com/juliasilge)

## Marco General

<center>
<img src = "https://www.tmwr.org/premade/data-science-model.svg" height = 450/>
</center>

## Estrategia de Validación

- La siguiente imagen fue tomada de [tidymodels.org](https://www.tidymodels.org/start/case-study/) e ilustra la estrategia de evaluación de modelos que adopté.

<center>
<img src = "https://www.tmwr.org/premade/resampling.svg" height = 450/>
</center>

## Fases del Modelado

<center>
<img src = "https://www.tmwr.org/premade/modeling-process.svg" height = 450/>
</center>

## Libros

### *Tidy Modeling with R*

- Consultar libro [aquí](https://www.tmwr.org/)

<center>
<img src = "../img/libro.PNG" height = 450/>
</center>

### *Applied Predictive Modeling*

- Consultar el libro [aquí.](http://appliedpredictivemodeling.com/)

<center>
<img src = "http://static1.squarespace.com/static/51156277e4b0b8b2ffe11c00/t/51157487e4b0b8b2ffe16829/1588028705660/?format=1500w" height = 450/>
</center>

### *Feature Engineering and Selection A Practical Approach for Predictive Models*

- Consultar libro [aquí.](http://www.feat.engineering/)

<center>
<img src = "https://images.routledge.com/common/jackets/amazon/978113807/9781138079229.jpg" height = 450/>
</center>

# Variables

<center>
<img src = "../img/variables.PNG" />
</center>

# Datos

```{r, warning=FALSE, message=FALSE}
# Cargando datos
library(tidyverse)
load("../data/my_train1.Rdata")
load("../data/my_test1.Rdata")
sampleSub <- read_csv("../data/sample_submission.csv")
head(new_train1)
```

- Selecciono sólo las variables que van a ingresar al análisis.

```{r}
mi_train <- new_train1 %>% 
  select(-c(App, date_update))

mi_test <- new_test1 %>% 
  select(-c(App, date_update))
```

- Primero se garantiza que los modelos sean entrenados con las mismas variables que contiene el archivo de test (submission).

```{r}
library(fastDummies)
# Binarización en test
binar_train <- dummy_cols(mi_train, remove_selected_columns = TRUE)
binar_test <- dummy_cols(mi_test, remove_selected_columns = TRUE)

# Las mismas variables en train y test (submission)
variables_iguales <- c(names(binar_train)[names(binar_train) %in% names(binar_test)],
                       "Rating")

# Datos de train finales
binar_train <- binar_train[, variables_iguales]
```

# Modelos

- [Documentación Catboost](https://catboost.ai/docs/concepts/about.html)
- [Biblioteca `treesnip`](https://curso-r.github.io/treesnip/index.html)
- [Biblioteca `treesnip` (Github)](https://github.com/curso-r/treesnip)
- Preprocesamiento:
  - Binarización (previamente binarizadas).
  - Imputación a través de *knn*. Este paso se podría obviar ya que el algoritmo maneja bien los valores ausentes.
  - Como es un problema de clasificación imbalanceado, utilizo la biblioteca [themis](https://github.com/tidymodels/themis) para generar clases balanceadas a través de muestreo con reemplazo (ascendnete - *upsample*).
- Pasos a seguir con *tidymodels*:
  - 0. División en *train*, *validación* y *test*
  - 1. Definir preprocesamiento
  - 2. Definir el modelo y los hiperparámetros a optimizar
  - 3. Definir la estrategia de validación del modelo
  - 4. Definir tipo de *tuning* (grid de hiperparámetros)
  - 5. Definir el frujo de trabajo (*workflow* en *tidymodels*)
  - 6. Ejecución o entrenamiento de modelos (*tuning*)
  - 7. Evaluación de modelos (gráficos con métricas de error)
  - 8. Ajuste del modelo final
  - 9. Predicciones finales
   
# Etapas del modelo

## Etapa 0 - Split

```{r}
# Tidymodels
library(tidymodels)

# Train-Test
set.seed(2020)
data_split <- initial_split(data = binar_train, prop = 0.80)
data_train <- training(data_split) %>% mutate(Rating = as.factor(Rating))
data_test <- testing(data_split) %>% mutate(Rating = as.factor(Rating))
```

## Etapa 1 - Preprocesamiento

```{r}
library(themis)
receta1 <- recipe(Rating ~ ., data = data_train) %>%
  step_knnimpute(all_predictors(), neighbors = 2) %>% 
  step_upsample(Rating)
```

## Etapa 2 - Modelos

- [Ajuste de modelos *Catboost* y *LightGBM* con `treesnip`](https://curso-r.github.io/treesnip/articles/working-with-lightgbm-catboost.html)

```{r}
# Modelo Catboost
library(catboost)
library(treesnip)
mod_catb <- boost_tree(
  mode = "classification",
  mtry = tune(),
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = 0.01,
  sample_size = tune()
) %>% 
  set_engine("catboost", nthread = 1)
```

## Etapa 3 - Validación

```{r}
set.seed(1234)
cv_config <- vfold_cv(data = data_train, 
                      v = 10,
                      strata = Rating)
```

## Etapa 4 - Grid Tuning

- En este caso voy a usar máxima entropia de tamaño 100.

```{r}
# Hiperparámetros Modelo Catboost
set.seed(12345)
catb_params <- parameters(
  finalize(mtry(), x = data_train[, -96]),
  trees(),
  min_n(),
  tree_depth(range = c(4, 10)),
  sample_size = sample_prop()
)

catb_grid <- grid_max_entropy(catb_params,
                              size = 100)
```

## Etapa 5 - *Workflow*

```{r}
# Workflow Modelo LightGBM
catb_wflow <- workflow() %>% 
  add_recipe(receta1) %>% 
  add_model(mod_catb)
```

## Etapa 6 - *Tuning*

- **Tuning LightGBM:** tiempo de ejecución aproximado de 2 horas.

```{r}
# Dependencias
#set_dependency("boost_tree", eng = "lightgbm", "lightgbm")
#set_dependency("boost_tree", eng = "lightgbm", "treesnip")

# Tiempos
library(tictoc)
tic() # Tiempo 0

#doParallel::registerDoParallel() # Inicio Paralelización
library(doMC)
registerDoMC(cores = 1)

# Tuning
catb_tuned <- tune_grid(
  object = catb_wflow,
  resamples = cv_config,
  grid = catb_grid,
  metrics = metric_set(roc_auc, f_meas),
  control = control_grid(save_pred = TRUE)
)

#doParallel::stopImplicitCluster() # Fin Paralelización

toc() # Tiempo final
```

## Etapa 7 - Evaluación de modelos

- **Mejores parámetros con f1 score:**

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "f_meas") %>% 
  arrange(desc(mean))
```

```{r}
show_best(catb_tuned, metric = "f_meas")
```

```{r}
select_best(catb_tuned, metric = "f_meas")
```


- **Mejores parámetros con roc_auc:**

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  arrange(desc(mean))
```

```{r}
show_best(catb_tuned, metric = "roc_auc")
```

```{r}
select_best(catb_tuned, metric = "roc_auc")
```

- En este caso los mejores modelos con ambas métricas resultaron ser diferentes. Utilizo predicciones con ambos.

### Hiperparámetros ROC-AUC

- **Tabla resumen:**

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  arrange(desc(mean))
```

- **Hiperparámetros:**

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = trees, y = mtry, size = min_n, color = mean)) +
  geom_point() +
  theme_light() +
  scale_color_viridis_c()
```

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = trees, y = mtry, size = tree_depth, color = mean)) +
  geom_point() +
  theme_light() +
  scale_color_viridis_c()
```

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = trees, y = mtry, size = sample_size, color = mean)) +
  geom_point() +
  theme_light() +
  scale_color_viridis_c()
```

### Desemepeño

- **Ajuste con mejores hiperparámetros:**

```{r}
# Mejores parámetros
mejor_roc <- select_best(catb_tuned, metric = "roc_auc")

# Finalizando workflow
modelo_final1 <- finalize_workflow(
  x = catb_wflow,
  parameters = mejor_roc
)

modelo_final1_fit <- modelo_final1 %>% 
  fit(data = data_train)
```

- **Predicciones en Train:**

```{r}
# Clases predichas en train
clases_train <- modelo_final1_fit %>% 
  predict(new_data = data_train, type = "class") 

# Probabilidades predichas en train
probs_train <- modelo_final1_fit %>% 
  predict(new_data = data_train, type = "prob")

# Reales, clases predichas y probabilidades predichas
predichos_train <- bind_cols(clases_train, probs_train) %>% 
  mutate(real = data_train$Rating)

head(predichos_train)
```

- **Matriz de confusión en Train:**

```{r}
predichos_train %>% 
  conf_mat(real, .pred_class) %>% 
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)
```

- **Precisión en Train:**

```{r}
predichos_train %>% 
  accuracy(real, .pred_class)
```

- **Curva ROC:**

```{r}
predichos_train %>%
  roc_curve(real, .pred_0) %>%
  autoplot() +
  geom_text(aes(
    x = 0.25,
    y = 1,
    label = paste0(
      "AUC: ",
      predichos_train %>%
        roc_auc(real, .pred_0) %>%
        pull(.estimate) %>%
        round(digits = 4)
    )
  ))
```

- **Predicciones en Validación:**

```{r}
# Clases predichas en train
clases_val <- modelo_final1_fit %>% 
  predict(new_data = data_test, type = "class") 

# Probabilidades predichas en train
probs_val <- modelo_final1_fit %>% 
  predict(new_data = data_test, type = "prob")

# Reales, clases predichas y probabilidades predichas
predichos_val <- bind_cols(clases_val, probs_val) %>% 
  mutate(real = data_test$Rating)

head(predichos_val)
```

- **Matriz de confusión en Validación:**

```{r}
predichos_val%>% 
  conf_mat(real, .pred_class) %>% 
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)
```

- **Precisión en Train:**

```{r}
predichos_val %>% 
  accuracy(real, .pred_class)
```

- **Curva ROC:**

```{r}
predichos_val %>%
  roc_curve(real, .pred_0) %>%
  autoplot() +
  geom_text(aes(
    x = 0.25,
    y = 1,
    label = paste0(
      "AUC: ",
      predichos_val %>%
        roc_auc(real, .pred_0) %>%
        pull(.estimate) %>%
        round(digits = 4)
    )
  ))
```

### Hiperparámetros  F1 Score

- **Tabla resumen:**

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "f_meas") %>% 
  arrange(desc(mean))
```

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "f_meas") %>% 
  ggplot(aes(x = trees, y = mtry, size = min_n, color = mean)) +
  geom_point() +
  theme_light() +
  scale_color_viridis_c()
```

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "f_meas") %>% 
  ggplot(aes(x = trees, y = mtry, size = tree_depth, color = mean)) +
  geom_point() +
  theme_light() +
  scale_color_viridis_c()
```

```{r}
catb_tuned %>% 
  collect_metrics() %>% 
  filter(.metric == "f_meas") %>% 
  ggplot(aes(x = trees, y = mtry, size = sample_size, color = mean)) +
  geom_point() +
  theme_light() +
  scale_color_viridis_c()
```

### Desemepeño

- **Ajuste con mejores hiperparámetros:**

```{r}
# Mejores parámetros
mejor_f1 <- select_best(catb_tuned, metric = "f_meas")

# Finalizando workflow
modelo_final2 <- finalize_workflow(
  x = catb_wflow,
  parameters = mejor_f1
)

modelo_final2_fit <- modelo_final2 %>% 
  fit(data = data_train)
```

- **Predicciones en Train:**

```{r}
# Clases predichas en train
clases_train2 <- modelo_final2_fit %>% 
  predict(new_data = data_train, type = "class") 

# Probabilidades predichas en train
probs_train2 <- modelo_final2_fit %>% 
  predict(new_data = data_train, type = "prob")

# Reales, clases predichas y probabilidades predichas
predichos_train2 <- bind_cols(clases_train2, probs_train2) %>% 
  mutate(real = data_train$Rating)

head(predichos_train2)
```

- **Matriz de confusión en Train:**

```{r}
predichos_train2 %>% 
  conf_mat(real, .pred_class) %>% 
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)
```

- **Precisión en Train:**

```{r}
predichos_train2 %>% 
  accuracy(real, .pred_class)
```

- **Curva ROC:**

```{r}
predichos_train2 %>%
  roc_curve(real, .pred_0) %>%
  autoplot() +
  geom_text(aes(
    x = 0.25,
    y = 1,
    label = paste0(
      "AUC: ",
      predichos_train %>%
        roc_auc(real, .pred_0) %>%
        pull(.estimate) %>%
        round(digits = 4)
    )
  ))
```

- **Predicciones en Validación:**

```{r}
# Clases predichas en train
clases_val2 <- modelo_final2_fit %>% 
  predict(new_data = data_test, type = "class") 

# Probabilidades predichas en train
probs_val2 <- modelo_final2_fit %>% 
  predict(new_data = data_test, type = "prob")

# Reales, clases predichas y probabilidades predichas
predichos_val2 <- bind_cols(clases_val2, probs_val2) %>% 
  mutate(real = data_test$Rating)

head(predichos_val2)
```

- **Matriz de confusión en Validación:**

```{r}
predichos_val2 %>% 
  conf_mat(real, .pred_class) %>% 
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)
```

- **Precisión en Train:**

```{r}
predichos_val2 %>% 
  accuracy(real, .pred_class)
```

- **Curva ROC:**

```{r}
predichos_val2 %>%
  roc_curve(real, .pred_0) %>%
  autoplot() +
  geom_text(aes(
    x = 0.25,
    y = 1,
    label = paste0(
      "AUC: ",
      predichos_val %>%
        roc_auc(real, .pred_0) %>%
        pull(.estimate) %>%
        round(digits = 4)
    )
  ))
```


# Modelo Final ROC

```{r}
# Finalizando modelación
prep_final <- prep(x = receta1)
data_final <- bake(object = prep_final, 
                   new_data = binar_train %>% mutate(Rating = as.factor(Rating)))

modelo_completo <- modelo_final1 %>% 
  fit(data = data_final)
```

# Submission

- **Predicciones finales:**

```{r}
# Receta sobre datos de test
test_final <- bake(object = prep_final, new_data = binar_test)

# Predicciones sobre test (submission)
predichos_final <- modelo_completo %>%
  predict(new_data = test_final, type = "class")

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_final$.pred_class) ->
  sub_69_catb_roc
head(sub_69_catb_roc)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_69_catb_roc, file = "../submission/sub_69_catb_roc.csv")
```

# Probabilidades

- Este modelo tuvo buen rendimiento en el tablero de la competencia. Voy a analizar cómo es la distribución de las probabilidades predichas.
- Hasta ahora sólo obtuve predicciones directas (*hard*), es decir, la clase o etiqueta final. Asumiendo el límite de probabilidad por defecto, que es 0.5.
- La calibración de modelos podría suministrar mejores resultados. Con ello se podría obtener algún criterio estadístico para definir el punto de corte (*threshold*).

```{r}
# Predicciones sobre test (submission)
predichos_final2 <- modelo_completo %>%
  predict(new_data = test_final, type = "prob")
```

- **Distribución de probabilides predichas:**

```{r}
predichos_final2 %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value, fill = name, color = name)) +
  geom_density(alpha = 0.5)
```

- **Cambiando punto de corte a 0.55:**

```{r}
predichos_final2 %>% 
  mutate(rating = if_else(.pred_1 > 0.55, true = "1", false = "0")) ->
  predichos_probs

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_probs$rating) ->
  sub_70_catb_roc
head(sub_70_catb_roc)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_70_catb_roc, file = "../submission/sub_70_catb_roc.csv")
```

- **Cambiando punto de corte a 0.45:**

```{r}
predichos_final2 %>% 
  mutate(rating = if_else(.pred_1 > 0.45, true = "1", false = "0")) ->
  predichos_probs2

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_probs2$rating) ->
  sub_71_catb_roc
head(sub_71_catb_roc)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_71_catb_roc, file = "../submission/sub_71_catb_roc.csv")
```

- **Cambiando punto de corte a 0.40:**

```{r}
predichos_final2 %>% 
  mutate(rating = if_else(.pred_1 > 0.40, true = "1", false = "0")) ->
  predichos_probs3

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_probs3$rating) ->
  sub_72_catb_roc
head(sub_72_catb_roc)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_72_catb_roc, file = "../submission/sub_72_catb_roc.csv")
```

- **Cambiando punto de corte a 0.47:**

```{r}
predichos_final2 %>% 
  mutate(rating = if_else(.pred_1 > 0.47, true = "1", false = "0")) ->
  predichos_probs4

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_probs4$rating) ->
  sub_73_catb_roc
head(sub_73_catb_roc)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_73_catb_roc, file = "../submission/sub_73_catb_roc.csv")
```

# Exportando probabilidades

```{r}
write_csv(predichos_final2, file = "../probabilitys/prob_catb_roc.csv")
```

# Modelo Final F1

```{r}
modelo_completo2 <- modelo_final2 %>% 
  fit(data = data_final)
```

# Submission

- **Predicciones finales:**

```{r}
# Predicciones sobre test (submission)
predichos_final_f1 <- modelo_completo2 %>%
  predict(new_data = test_final, type = "class")

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_final_f1$.pred_class) ->
  sub_74_catb_f1
head(sub_74_catb_f1)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_74_catb_f1, file = "../submission/sub_74_catb_f1.csv")
```

# Probabilidades

- Este modelo tuvo buen rendimiento en el tablero de la competencia. Voy a analizar cómo es la distribución de las probabilidades predichas.
- Hasta ahora sólo obtuve predicciones directas (*hard*), es decir, la clase o etiqueta final. Asumiendo el límite de probabilidad por defecto, que es 0.5.
- La calibración de modelos podría suministrar mejores resultados. Con ello se podría obtener algún criterio estadístico para definir el punto de corte (*threshold*).

```{r}
# Predicciones sobre test (submission)
predichos_final2_f1 <- modelo_completo2 %>%
  predict(new_data = test_final, type = "prob")
```

- **Distribución de probabilides predichas:**

```{r}
predichos_final2_f1 %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value, fill = name, color = name)) +
  geom_density(alpha = 0.5)
```

- **Cambiando punto de corte a 0.55:**

```{r}
predichos_final2_f1 %>% 
  mutate(rating = if_else(.pred_1 > 0.55, true = "1", false = "0")) ->
  predichos_probs_f1

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_probs_f1$rating) ->
  sub_75_catb_f1
head(sub_75_catb_f1)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_75_catb_f1, file = "../submission/sub_75_catb_f1.csv")
```

- **Cambiando punto de corte a 0.45:**

```{r}
predichos_final2_f1 %>% 
  mutate(rating = if_else(.pred_1 > 0.45, true = "1", false = "0")) ->
  predichos_probs2_f1

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_probs2_f1$rating) ->
  sub_76_catb_f1
head(sub_76_catb_f1)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_76_catb_f1, file = "../submission/sub_76_catb_f1.csv")
```

- **Cambiando punto de corte a 0.40:**

```{r}
predichos_final2_f1 %>% 
  mutate(rating = if_else(.pred_1 > 0.40, true = "1", false = "0")) ->
  predichos_probs3_f1

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_probs3_f1$rating) ->
  sub_77_catb_f1
head(sub_77_catb_f1)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_77_catb_f1, file = "../submission/sub_77_catb_f1.csv")
```

- **Cambiando punto de corte a 0.47:**

```{r}
predichos_final2_f1 %>% 
  mutate(rating = if_else(.pred_1 > 0.47, true = "1", false = "0")) ->
  predichos_probs4_f1

# Submission
sampleSub %>% 
  select(-rating) %>% 
  mutate(rating = predichos_probs4_f1$rating) ->
  sub_78_catb_f1
head(sub_78_catb_f1)
```

- **Exportando predicciones:**

```{r}
write_csv(sub_78_catb_f1, file = "../submission/sub_78_catb_f1.csv")
```

# Exportando probabilidades

```{r}
write_csv(predichos_final2_f1, file = "../probabilitys/prob_catb_f1.csv")
```